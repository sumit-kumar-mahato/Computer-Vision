{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Objective: Traffic Monitoring\n",
        "\n",
        "- Red Light cameras --> Track vehicle trajectory after detection --. Determine it crossed line during red.\n",
        "- Highway congestion analysis --> Follow groups of cars --> estimate avergae speed, detect stopped vehicles.\n",
        "- Wrong way driving alert --. track detection --> if car goes opposie to flow --> alarm\n",
        "- Toll booth/APNR --> Track vehicle from far --> zoom inw hen close for plate reading."
      ],
      "metadata": {
        "id": "7lnQW8Q4_bBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "1. Import Video File\n",
        "2. Video Selection\n",
        "3. Create Frame\n",
        "4. Object tracking\n",
        "5. Drawing Box\n",
        "6. Testing\n",
        "\n"
      ],
      "metadata": {
        "id": "3kiEchjG9wyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup & Installation"
      ],
      "metadata": {
        "id": "mWnrs2sZBRaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics opencv-python-headless numpy matplotlib lap pytesseract"
      ],
      "metadata": {
        "id": "Ys5L_McmBU47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27589318-f0a0-4cf1-e558-f213eec0b0ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG2WYo-t8pOP",
        "outputId": "606286cb-072e-40fa-a8d6-f9c4d4bf048f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "import pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 2. Configuration (Edit Only This Cell)\n",
        "# =========================================\n",
        "\n",
        "class CFG:\n",
        "    # --- Model ---\n",
        "    yolo_model = \"yolov8n.pt\"   # you can change to 'yolov8s.pt' etc.\n",
        "    tracker_cfg = \"bytetrack.yaml\"  # or 'botsort.yaml' [web:13]\n",
        "\n",
        "    # --- Paths (update with your files) ---\n",
        "    # Place your videos in /content and set names here:\n",
        "    red_light_video = \"/content/4.mp4\"\n",
        "    highway_video   = \"/content/4.mp4\"\n",
        "    wrong_way_video = \"/content/4.mp4\"\n",
        "    toll_video      = \"/content/4.mp4\"\n",
        "\n",
        "    # --- General Detection Filters ---\n",
        "    vehicle_classes = [2, 3, 5, 7]  # YOLO COCO: car, motorcycle, bus, truck (id depends on model) [web:19]\n",
        "\n",
        "    conf_thres = 0.4\n",
        "    iou_thres  = 0.5\n",
        "\n",
        "    # --- Red-light Configuration ---\n",
        "    # Stop-line: (x1, y1, x2, y2) in image pixel coords\n",
        "    stop_line = (200, 400, 1000, 400)  # example; adjust per video\n",
        "    # Region where traffic light is located (x1, y1, x2, y2)\n",
        "    signal_roi = (20, 20, 150, 150)\n",
        "    # For simple demo: manually set current signal state if you don't have a trained light-color model\n",
        "    # options: \"red\", \"yellow\", \"green\"\n",
        "    manual_signal_state = \"red\"\n",
        "\n",
        "    # --- Highway Speed Estimation ---\n",
        "    # Reference line for speed measurement (entering and leaving line)\n",
        "    enter_line = (200, 350, 1000, 350)\n",
        "    exit_line  = (200, 450, 1000, 450)\n",
        "    # Approx real-world distance between enter_line and exit_line (meters)\n",
        "    distance_m = 30.0\n",
        "    fps_assumed = 30.0  # update if you know true fps\n",
        "\n",
        "    stopped_speed_thresh = 1.0  # km/h considered 'stopped'\n",
        "    stopped_frame_thresh = 30   # frames where speed < threshold to mark stopped\n",
        "\n",
        "    # --- Wrong-way Driving ---\n",
        "    # Assume allowed direction is \"top-to-bottom\" (dy > 0).\n",
        "    # If majority motion is reversed, raise alarm.\n",
        "    wrong_way_min_frames = 15\n",
        "\n",
        "    # --- Toll / ANPR ---\n",
        "    # ROI in which we zoom when vehicle close to booth\n",
        "    toll_close_roi = (500, 300, 900, 700)  # example\n",
        "\n",
        "    # Plate OCR config (for pytesseract)\n",
        "    tesseract_config = \"--oem 3 --psm 7\"\n",
        "\n",
        "cfg = CFG()\n",
        "print(\"Config loaded. Edit CFG class above for customization.\")\n"
      ],
      "metadata": {
        "id": "EawLM6SsBYUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1182d94f-2483-48a6-cb32-b3f3e63d855a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config loaded. Edit CFG class above for customization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 3. Utility Functions (Geometry & Helpers)\n",
        "# =========================================\n",
        "\n",
        "def line_intersection_y(p1, p2, y_line):\n",
        "    # returns x at which a line from p1 to p2 crosses horizontal y_line (if any)\n",
        "    (x1, y1), (x2, y2) = p1, p2\n",
        "    if (y1 < y_line and y2 < y_line) or (y1 > y_line and y2 > y_line) or (y1 == y2):\n",
        "        return None\n",
        "    t = (y_line - y1) / (y2 - y1)\n",
        "    x_int = x1 + t * (x2 - x1)\n",
        "    return x_int\n",
        "\n",
        "def is_crossing_line(prev_center, center, line):\n",
        "    x1, y1, x2, y2 = line\n",
        "    if y1 == y2:  # horizontal line\n",
        "        y_line = y1\n",
        "        y_prev, y_curr = prev_center[1], center[1]\n",
        "        if (y_prev < y_line and y_curr >= y_line) or (y_prev > y_line and y_curr <= y_line):\n",
        "            return True\n",
        "    else:\n",
        "        # approximate using bounding box center projection if needed\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "def draw_line(img, line, color=(0, 255, 255), thickness=2, label=None):\n",
        "    x1, y1, x2, y2 = line\n",
        "    cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
        "    if label:\n",
        "        cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "def crop_roi(frame, roi):\n",
        "    x1, y1, x2, y2 = roi\n",
        "    return frame[y1:y2, x1:x2]\n",
        "\n",
        "def put_centered_text(img, text, y=50, color=(0,0,255), scale=1.0, thickness=2):\n",
        "    w = img.shape[1]\n",
        "    (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, scale, thickness)\n",
        "    x = (w - tw) // 2\n",
        "    cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, scale, color, thickness)\n"
      ],
      "metadata": {
        "id": "m66lEAWmOcgs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 4. Load YOLO Model (with Tracker)\n",
        "# =========================================\n",
        "\n",
        "model = YOLO(cfg.yolo_model)\n",
        "print(\"YOLO model loaded:\", cfg.yolo_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyIKl4p6OxYx",
        "outputId": "b6f994a3-a6a3-4e3f-f068-a88b1dcce6bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 144.0MB/s 0.0s\n",
            "YOLO model loaded: yolov8n.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 5. Base Detection + Tracking Wrapper\n",
        "# =========================================\n",
        "# Ultralytics provides built-in multi-object tracking with BoT-SORT / ByteTrack. [web:13]\n",
        "\n",
        "def yolo_track_video(\n",
        "    video_path,\n",
        "    tracker_cfg=cfg.tracker_cfg,\n",
        "    show=False,\n",
        "    save_output_path=None,\n",
        "    process_frame_fn=None\n",
        "):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Failed to open video:\", video_path)\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or cfg.fps_assumed\n",
        "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    if save_output_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        out = cv2.VideoWriter(save_output_path, fourcc, fps, (width, height))\n",
        "    else:\n",
        "        out = None\n",
        "\n",
        "    frame_idx = 0\n",
        "\n",
        "    # state dictionary you can use inside process_frame_fn\n",
        "    state = {\n",
        "        \"fps\": fps,\n",
        "        \"frame_idx\": 0,\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        state[\"frame_idx\"] = frame_idx\n",
        "\n",
        "        # YOLO tracking call. 'track' returns tracked boxes with IDs. [web:13]\n",
        "        results = model.track(\n",
        "            frame,\n",
        "            tracker=tracker_cfg,\n",
        "            conf=cfg.conf_thres,\n",
        "            iou=cfg.iou_thres,\n",
        "            persist=True,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        if len(results) > 0:\n",
        "            r = results[0]\n",
        "        else:\n",
        "            r = None\n",
        "\n",
        "        # process_frame_fn can annotate frame, update metrics, etc.\n",
        "        if process_frame_fn is not None:\n",
        "            frame = process_frame_fn(frame, r, state)\n",
        "\n",
        "        if out is not None:\n",
        "            out.write(frame)\n",
        "\n",
        "        if show:\n",
        "            cv2.imshow(\"Traffic Monitoring\", frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "    cap.release()\n",
        "    if out is not None:\n",
        "        out.release()\n",
        "    if show:\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"Done:\", video_path, \"->\", save_output_path)\n"
      ],
      "metadata": {
        "id": "fNU8SyrzOy6Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 6. Red-Light Violation Detection\n",
        "# =========================================\n",
        "# Idea [web:14][web:21][web:12]:\n",
        "# - Track vehicles with IDs.\n",
        "# - Maintain their center positions.\n",
        "# - When signal is red and trajectory crosses the stop-line from \"before\" to \"after\", log violation.\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def red_light_process_frame(frame, result, state):\n",
        "    stop_line = cfg.stop_line\n",
        "    draw_line(frame, stop_line, (0, 255, 255), 2, \"STOP LINE\")\n",
        "\n",
        "    # For now, use manual signal state; you can replace with a trained light-color detector. [web:8][web:12]\n",
        "    signal_state = cfg.manual_signal_state\n",
        "    x1, y1, x2, y2 = cfg.signal_roi\n",
        "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255 if signal_state == \"red\" else 0), 2)\n",
        "    cv2.putText(frame, f\"Signal: {signal_state.upper()}\", (x1, y1-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
        "                (0, 0, 255 if signal_state == \"red\" else (0,255,0)), 2)\n",
        "\n",
        "    if \"track_history\" not in state:\n",
        "        state[\"track_history\"] = defaultdict(list)\n",
        "    if \"violations\" not in state:\n",
        "        state[\"violations\"] = set()\n",
        "\n",
        "    track_history = state[\"track_history\"]\n",
        "    violations = state[\"violations\"]\n",
        "\n",
        "    if result is not None and result.boxes is not None:\n",
        "        boxes = result.boxes\n",
        "        for box in boxes:\n",
        "            cls = int(box.cls[0])\n",
        "            if cls not in cfg.vehicle_classes:\n",
        "                continue\n",
        "            id_ = int(box.id[0]) if box.id is not None else None\n",
        "            if id_ is None:\n",
        "                continue\n",
        "\n",
        "            x1b, y1b, x2b, y2b = map(int, box.xyxy[0])\n",
        "            cx, cy = int((x1b + x2b) / 2), int((y1b + y2b) / 2)\n",
        "\n",
        "            # draw box and id\n",
        "            cv2.rectangle(frame, (x1b, y1b), (x2b, y2b), (0, 255, 0), 2)\n",
        "            cv2.circle(frame, (cx, cy), 3, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID {id_}\", (x1b, y1b-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "            # update history\n",
        "            history = track_history[id_]\n",
        "            history.append((cx, cy))\n",
        "            if len(history) > 2:\n",
        "                prev = history[-2]\n",
        "                # check crossing\n",
        "                if signal_state == \"red\" and is_crossing_line(prev, (cx, cy), stop_line):\n",
        "                    if id_ not in violations:\n",
        "                        violations.add(id_)\n",
        "                        cv2.putText(frame, \"VIOLATION!\", (x1b, y1b-30),\n",
        "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "    # overlay count\n",
        "    cv2.putText(frame, f\"Violations: {len(state['violations'])}\", (20, state[\"height\"]-20),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "def run_red_light_module():\n",
        "    print(\"Running Red-Light Violation Detection...\")\n",
        "    yolo_track_video(\n",
        "        cfg.red_light_video,\n",
        "        tracker_cfg=cfg.tracker_cfg,\n",
        "        show=False,\n",
        "        save_output_path=\"/content/red_light_output.mp4\",\n",
        "        process_frame_fn=red_light_process_frame\n",
        "    )\n",
        "\n",
        "run_red_light_module()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Slkhz7vO_3V",
        "outputId": "0bc0699a-1185-4064-9d67-533733018e72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Red-Light Violation Detection...\n",
            "Done: /content/4.mp4 -> /content/red_light_output.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 7. Highway Congestion & Speed Estimation\n",
        "# =========================================\n",
        "# Idea [web:11][web:17][web:20]:\n",
        "# - Maintain entry & exit timestamps per track when crossing two lines.\n",
        "# - Estimate speed = distance / time.\n",
        "# - Detect stopped vehicles if speed below threshold for N frames.\n",
        "# - Compute rolling average speed to infer congestion.\n",
        "\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "def highway_process_frame(frame, result, state):\n",
        "    draw_line(frame, cfg.enter_line, (255, 0, 0), 2, \"ENTER\")\n",
        "    draw_line(frame, cfg.exit_line, (0, 255, 0), 2, \"EXIT\")\n",
        "\n",
        "    if \"track_history\" not in state:\n",
        "        state[\"track_history\"] = {}\n",
        "    if \"enter_time\" not in state:\n",
        "        state[\"enter_time\"] = {}\n",
        "    if \"exit_time\" not in state:\n",
        "        state[\"exit_time\"] = {}\n",
        "    if \"speeds_kmh\" not in state:\n",
        "        state[\"speeds_kmh\"] = {}\n",
        "    if \"stopped_frames\" not in state:\n",
        "        state[\"stopped_frames\"] = {}\n",
        "    if \"avg_speed_window\" not in state:\n",
        "        state[\"avg_speed_window\"] = deque(maxlen=100)\n",
        "\n",
        "    track_history = state[\"track_history\"]\n",
        "    enter_time = state[\"enter_time\"]\n",
        "    exit_time = state[\"exit_time\"]\n",
        "    speeds_kmh = state[\"speeds_kmh\"]\n",
        "    stopped_frames = state[\"stopped_frames\"]\n",
        "    avg_speed_window = state[\"avg_speed_window\"]\n",
        "\n",
        "    fps = state[\"fps\"]\n",
        "\n",
        "    if result is not None and result.boxes is not None:\n",
        "        for box in result.boxes:\n",
        "            cls = int(box.cls[0])\n",
        "            if cls not in cfg.vehicle_classes:\n",
        "                continue\n",
        "\n",
        "            id_ = int(box.id[0]) if box.id is not None else None\n",
        "            if id_ is None:\n",
        "                continue\n",
        "\n",
        "            x1b, y1b, x2b, y2b = map(int, box.xyxy[0])\n",
        "            cx, cy = int((x1b + x2b) / 2), int((y1b + y2b) / 2)\n",
        "\n",
        "            cv2.rectangle(frame, (x1b, y1b), (x2b, y2b), (255, 255, 0), 2)\n",
        "            cv2.putText(frame, f\"ID {id_}\", (x1b, y1b-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "            if id_ not in track_history:\n",
        "                track_history[id_] = []\n",
        "            track_history[id_].append((cx, cy))\n",
        "            hist = track_history[id_]\n",
        "            if len(hist) > 2:\n",
        "                prev = hist[-2]\n",
        "                # enter line\n",
        "                if is_crossing_line(prev, (cx, cy), cfg.enter_line) and id_ not in enter_time:\n",
        "                    enter_time[id_] = state[\"frame_idx\"]\n",
        "                # exit line\n",
        "                if is_crossing_line(prev, (cx, cy), cfg.exit_line) and id_ not in exit_time:\n",
        "                    exit_time[id_] = state[\"frame_idx\"]\n",
        "                    if id_ in enter_time:\n",
        "                        dt_frames = exit_time[id_] - enter_time[id_]\n",
        "                        dt_sec = dt_frames / fps\n",
        "                        if dt_sec > 0:\n",
        "                            speed_mps = cfg.distance_m / dt_sec\n",
        "                            speed_kmh = speed_mps * 3.6\n",
        "                            speeds_kmh[id_] = speed_kmh\n",
        "                            avg_speed_window.append(speed_kmh)\n",
        "\n",
        "            # stopped detection (using short baseline of history)\n",
        "            if len(hist) >= 5:\n",
        "                # displacement over last N points\n",
        "                x0, y0 = hist[-5]\n",
        "                dx = cx - x0\n",
        "                dy = cy - y0\n",
        "                dist_pix = np.sqrt(dx*dx + dy*dy)\n",
        "                # approximate \"speed\" in pixel/frame and threshold it\n",
        "                if id_ not in stopped_frames:\n",
        "                    stopped_frames[id_] = 0\n",
        "                if dist_pix < 2:  # low movement -> candidate stopped\n",
        "                    stopped_frames[id_] += 1\n",
        "                else:\n",
        "                    stopped_frames[id_] = 0\n",
        "\n",
        "                if stopped_frames[id_] >= cfg.stopped_frame_thresh:\n",
        "                    cv2.putText(frame, \"STOPPED\", (x1b, y2b+20),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
        "\n",
        "            # draw speed if known\n",
        "            if id_ in speeds_kmh:\n",
        "                cv2.putText(frame, f\"{speeds_kmh[id_]:.1f} km/h\", (x1b, y2b+40),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
        "\n",
        "    # global congestion metrics\n",
        "    if len(avg_speed_window) > 0:\n",
        "        avg_speed = np.mean(avg_speed_window)\n",
        "    else:\n",
        "        avg_speed = 0.0\n",
        "\n",
        "    congestion_text = f\"Avg speed: {avg_speed:.1f} km/h\"\n",
        "    cv2.putText(frame, congestion_text, (20, 40),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "def run_highway_module():\n",
        "    print(\"Running Highway Congestion & Speed...\")\n",
        "    yolo_track_video(\n",
        "        cfg.highway_video,\n",
        "        tracker_cfg=cfg.tracker_cfg,\n",
        "        show=False,\n",
        "        save_output_path=\"/content/highway_output.mp4\",\n",
        "        process_frame_fn=highway_process_frame\n",
        "    )\n",
        "\n",
        "run_highway_module()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m78NwK8BPElJ",
        "outputId": "e3bb27f1-6865-4d0e-c78a-9d1b1a38339c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Highway Congestion & Speed...\n",
            "Done: /content/4.mp4 -> /content/highway_output.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 8. Wrong-Way Driving Detection\n",
        "# =========================================\n",
        "# Idea [web:9][web:10][web:18][web:21]:\n",
        "# - For each track, accumulate last N centers.\n",
        "# - Compute average direction vector (dx, dy).\n",
        "# - If allowed direction is downwards (dy > 0) but average dy < 0 for long enough, mark as wrong-way.\n",
        "\n",
        "def wrong_way_process_frame(frame, result, state):\n",
        "    if \"track_history\" not in state:\n",
        "        state[\"track_history\"] = {}\n",
        "    if \"wrong_way_ids\" not in state:\n",
        "        state[\"wrong_way_ids\"] = set()\n",
        "\n",
        "    track_history = state[\"track_history\"]\n",
        "    wrong_way_ids = state[\"wrong_way_ids\"]\n",
        "\n",
        "    if result is not None and result.boxes is not None:\n",
        "        for box in result.boxes:\n",
        "            cls = int(box.cls[0])\n",
        "            if cls not in cfg.vehicle_classes:\n",
        "                continue\n",
        "\n",
        "            id_ = int(box.id[0]) if box.id is not None else None\n",
        "            if id_ is None:\n",
        "                continue\n",
        "\n",
        "            x1b, y1b, x2b, y2b = map(int, box.xyxy[0])\n",
        "            cx, cy = int((x1b + x2b) / 2), int((y1b + y2b) / 2)\n",
        "\n",
        "            if id_ not in track_history:\n",
        "                track_history[id_] = []\n",
        "            track_history[id_].append((cx, cy))\n",
        "            # keep last 30 points\n",
        "            track_history[id_] = track_history[id_][-30:]\n",
        "\n",
        "            hist = track_history[id_]\n",
        "            if len(hist) >= cfg.wrong_way_min_frames:\n",
        "                x0, y0 = hist[0]\n",
        "                x_last, y_last = hist[-1]\n",
        "                dx = x_last - x0\n",
        "                dy = y_last - y0\n",
        "\n",
        "                # Allowed direction: downwards (dy > 0)\n",
        "                if dy < -10:  # significant upward movement\n",
        "                    wrong_way_ids.add(id_)\n",
        "\n",
        "            color = (0, 255, 0)\n",
        "            if id_ in wrong_way_ids:\n",
        "                color = (0, 0, 255)\n",
        "                cv2.putText(frame, \"WRONG WAY!\", (x1b, y1b-30),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
        "\n",
        "            cv2.rectangle(frame, (x1b, y1b), (x2b, y2b), color, 2)\n",
        "            cv2.putText(frame, f\"ID {id_}\", (x1b, y1b-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
        "\n",
        "    put_centered_text(frame, f\"Wrong-way IDs: {len(state['wrong_way_ids'])}\",\n",
        "                      y=40, color=(0,0,255))\n",
        "\n",
        "    return frame\n",
        "\n",
        "def run_wrong_way_module():\n",
        "    print(\"Running Wrong-Way Driving Detection...\")\n",
        "    yolo_track_video(\n",
        "        cfg.wrong_way_video,\n",
        "        tracker_cfg=cfg.tracker_cfg,\n",
        "        show=False,\n",
        "        save_output_path=\"/content/wrong_way_output.mp4\",\n",
        "        process_frame_fn=wrong_way_process_frame\n",
        "    )\n",
        "\n",
        "run_wrong_way_module()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGT_N4N6Pm5f",
        "outputId": "01ca93c6-d58b-4937-fd8f-9b7848585978"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Wrong-Way Driving Detection...\n",
            "Done: /content/4.mp4 -> /content/wrong_way_output.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 9. Toll Booth / ANPR (Zoom + Plate OCR)\n",
        "# =========================================\n",
        "# Idea [web:12][web:21]:\n",
        "# - Track leading vehicle approaching booth.\n",
        "# - When its bounding box center is inside toll_close_roi, crop area around vehicle.\n",
        "# - Run plate detector (could be YOLO plate model or classical methods) + OCR.\n",
        "# - For demo: simple bbox-based zoom + OCR on whole bbox.\n",
        "\n",
        "def toll_anpr_process_frame(frame, result, state):\n",
        "    x1r, y1r, x2r, y2r = cfg.toll_close_roi\n",
        "    cv2.rectangle(frame, (x1r, y1r), (x2r, y2r), (255, 0, 0), 2)\n",
        "    cv2.putText(frame, \"TOLL ROI\", (x1r, y1r-10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
        "\n",
        "    if \"last_plate\" not in state:\n",
        "        state[\"last_plate\"] = \"\"\n",
        "\n",
        "    if result is not None and result.boxes is not None:\n",
        "        for box in result.boxes:\n",
        "            cls = int(box.cls[0])\n",
        "            if cls not in cfg.vehicle_classes:\n",
        "                continue\n",
        "\n",
        "            id_ = int(box.id[0]) if box.id is not None else None\n",
        "            if id_ is None:\n",
        "                continue\n",
        "\n",
        "            x1b, y1b, x2b, y2b = map(int, box.xyxy[0])\n",
        "            cx, cy = int((x1b + x2b) / 2), int((y1b + y2b) / 2)\n",
        "\n",
        "            cv2.rectangle(frame, (x1b, y1b), (x2b, y2b), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, f\"ID {id_}\", (x1b, y1b-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
        "\n",
        "            # if vehicle center is inside toll ROI, zoom and OCR\n",
        "            if x1r < cx < x2r and y1r < cy < y2r:\n",
        "                veh_crop = frame[y1b:y2b, x1b:x2b]\n",
        "                if veh_crop.size == 0:\n",
        "                    continue\n",
        "\n",
        "                # Optional: resize crop for better OCR\n",
        "                scale = 3\n",
        "                veh_zoom = cv2.resize(veh_crop, None, fx=scale, fy=scale,\n",
        "                                      interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                # For proper ANPR you should train a plate detector. [web:12][web:21]\n",
        "                # Here we run OCR on whole zoomed vehicle bbox as a placeholder.\n",
        "                gray = cv2.cvtColor(veh_zoom, cv2.COLOR_BGR2GRAY)\n",
        "                _, thr = cv2.threshold(gray, 0, 255,\n",
        "                                       cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "                text = pytesseract.image_to_string(thr, config=cfg.tesseract_config)\n",
        "                text = \"\".join(ch for ch in text if ch.isalnum()).upper()\n",
        "\n",
        "                if text:\n",
        "                    state[\"last_plate\"] = text\n",
        "                    cv2.putText(frame, text, (x1b, y2b+20),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
        "\n",
        "    if state[\"last_plate\"]:\n",
        "        put_centered_text(frame, f\"Last plate: {state['last_plate']}\",\n",
        "                          y=40, color=(0,255,0))\n",
        "\n",
        "    return frame\n",
        "\n",
        "def run_toll_module():\n",
        "    print(\"Running Toll / ANPR module...\")\n",
        "    yolo_track_video(\n",
        "        cfg.toll_video,\n",
        "        tracker_cfg=cfg.tracker_cfg,\n",
        "        show=False,\n",
        "        save_output_path=\"/content/toll_output.mp4\",\n",
        "        process_frame_fn=toll_anpr_process_frame\n",
        "    )\n",
        "\n",
        "run_toll_module()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmH9bHDSPqaO",
        "outputId": "64a1ccba-3e59-42f0-867e-8862b4c5f1f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Toll / ANPR module...\n",
            "Done: /content/4.mp4 -> /content/toll_output.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJ5SMrZ8PsE9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}